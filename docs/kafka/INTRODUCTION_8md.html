<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.5"/>
<title>librdkafka: INTRODUCTION.md File Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">librdkafka
   </div>
   <div id="projectbrief">The Apache Kafka C/C++ client library</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.5 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="annotated.html"><span>Data&#160;Structures</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="files.html"><span>File&#160;List</span></a></li>
      <li><a href="globals.html"><span>Globals</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('INTRODUCTION_8md.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Data Structures</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&#160;</span>Macros</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(9)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">INTRODUCTION.md File Reference</div>  </div>
</div><!--header-->
<div class="contents">
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><h1>Introduction to librdkafka - the Apache Kafka C/C++ client library</h1>
<p>librdkafka is a high performance C implementation of the Apache Kafka client, providing a reliable and performant client for production use. librdkafka also provides a native C++ interface.</p>
<h2>Contents</h2>
<p>The following chapters are available in this document</p>
<ul>
<li>Performance<ul>
<li>Performance numbers</li>
<li>High throughput</li>
<li>Low latency</li>
<li>Compression</li>
</ul>
</li>
<li>Message reliability</li>
<li>Usage<ul>
<li>Documentation</li>
<li>Initialization</li>
<li>Configuration</li>
<li>Threads and callbacks</li>
<li>Brokers</li>
<li>Producer API</li>
<li>Consumer API</li>
</ul>
</li>
<li>Appendix<ul>
<li>Test detailts</li>
</ul>
</li>
</ul>
<h2>Performance</h2>
<p>librdkafka is a multi-threaded library designed for use on modern hardware and it attempts to keep memory copying at a minimal. The payload of produced or consumed messages may pass through without any copying (if so desired by the application) putting no limit on message sizes.</p>
<p>librdkafka allows you to decide if high throughput is the name of the game, or if a low latency service is required, all through the configuration property interface.</p>
<p>The two most important configuration properties for performance tuning are:</p>
<ul>
<li>batch.num.messages - the minimum number of messages to wait for to accumulate in the local queue before sending off a message set.</li>
<li>queue.buffering.max.ms - how long to wait for batch.num.messages to fill up in the local queue.</li>
</ul>
<h3>Performance numbers</h3>
<p>The following performance numbers stem from tests using the following setup:</p>
<ul>
<li>Intel Quad Core i7 at 3.4GHz, 8GB of memory</li>
<li>Disk performance has been shortcut by setting the brokers' flush configuration properties as so:<ul>
<li><code>log.flush.interval.messages=10000000</code></li>
<li><code>log.flush.interval.ms=100000</code></li>
</ul>
</li>
<li>Two brokers running on the same machine as librdkafka.</li>
<li>One topic with two partitions.</li>
<li>Each broker is leader for one partition each.</li>
<li>Using <code>rdkafka_performance</code> program available in the <code>examples</code> subdir.</li>
</ul>
<p><b>Test results</b></p>
<ul>
<li><b>Test1</b>: 2 brokers, 2 partitions, required.acks=2, 100 byte messages: <b>850000 messages/second</b>, <b>85 MB/second</b></li>
<li><b>Test2</b>: 1 broker, 1 partition, required.acks=0, 100 byte messages: <b>710000 messages/second</b>, <b>71 MB/second</b></li>
<li><b>Test3</b>: 2 broker2, 2 partitions, required.acks=2, 100 byte messages, snappy compression: <b>300000 messages/second</b>, <b>30 MB/second</b></li>
<li><b>Test4</b>: 2 broker2, 2 partitions, required.acks=2, 100 byte messages, gzip compression: <b>230000 messages/second</b>, <b>23 MB/second</b></li>
</ul>
<p><b>Note</b>: See the <em>Test details</em> chapter at the end of this document for information about the commands executed, etc.</p>
<p><b>Note</b>: Consumer performance tests will be announced soon.</p>
<h3>High throughput</h3>
<p>The key to high throughput is message batching - waiting for a certain amount of messages to accumulate in the local queue before sending them off in one large message set or batch to the peer. This amortizes the messaging overhead and eliminates the adverse effect of the round trip time (rtt).</p>
<p>The default settings, batch.num.messages=10000 and queue.buffering.max.ms=1000, are suitable for high throughput. This allows librdkafka to wait up to 1000 ms for up to 10000 messages to accumulate in the local queue before sending the accumulate messages to the broker.</p>
<p>These setting are set globally (<code>rd_kafka_conf_t</code>) but applies on a per topic+partition basis.</p>
<h3>Low latency</h3>
<p>When low latency messaging is required the "queue.buffering.max.ms" should be tuned to the maximum permitted producer-side latency. Setting queue.buffering.max.ms to 1 will make sure messages are sent as soon as possible. You could check out <a href="https://github.com/edenhill/librdkafka/wiki/How-to-decrease-message-latency">How to decrease message latency</a> to find more details.</p>
<h3>Compression</h3>
<p>Producer message compression is enabled through the "compression.codec" configuration property.</p>
<p>Compression is performed on the batch of messages in the local queue, the larger the batch the higher likelyhood of a higher compression ratio. The local batch queue size is controlled through the "batch.num.messages" and "queue.buffering.max.ms" configuration properties as described in the <b>High throughput</b> chapter above.</p>
<h2>Message reliability</h2>
<p>Message reliability is an important factor of librdkafka - an application can rely fully on librdkafka to deliver a message according to the specified configuration ("request.required.acks" and "message.send.max.retries", etc).</p>
<p>If the topic configuration property "request.required.acks" is set to wait for message commit acknowledgements from brokers (any value but 0, see <a href="https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md">`CONFIGURATION.md`</a> for specifics) then librdkafka will hold on to the message until all expected acks have been received, gracefully handling the following events:</p>
<ul>
<li>Broker connection failure</li>
<li>Topic leader change</li>
<li>Produce errors signaled by the broker</li>
</ul>
<p>This is handled automatically by librdkafka and the application does not need to take any action at any of the above events. The message will be resent up to "message.send.max.retries" times before reporting a failure back to the application.</p>
<p>The delivery report callback is used by librdkafka to signal the status of a message back to the application, it will be called once for each message to report the status of message delivery:</p>
<ul>
<li>If <code>error_code</code> is non-zero the message delivery failed and the error_code indicates the nature of the failure (<code>rd_kafka_resp_err_t</code> enum).</li>
<li>If <code>error_code</code> is zero the message has been successfully delivered.</li>
</ul>
<p>See Producer API chapter for more details on delivery report callback usage.</p>
<p>The delivery report callback is optional.</p>
<h2>Usage</h2>
<h3>Documentation</h3>
<p>The librdkafka API is documented in the <a href="https://github.com/edenhill/librdkafka/blob/master/src/rdkafka.h">`rdkafka.h`</a> header file, the configuration properties are documented in <a href="https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md">`CONFIGURATION.md`</a></p>
<h3>Initialization</h3>
<p>The application needs to instantiate a top-level object <code>rd_kafka_t</code> which is the base container, providing global configuration and shared state. It is created by calling <code><a class="el" href="rdkafka_8h.html#a63d5cd86ab1f77772b2be170e1c09c24" title="Creates a new Kafka handle and starts its operation according to the specified type (RD_KAFKA_CONSUME...">rd_kafka_new()</a></code>.</p>
<p>It also needs to instantiate one or more topics (<code>rd_kafka_topic_t</code>) to be used for producing to or consuming from. The topic object holds topic-specific configuration and will be internally populated with a mapping of all available partitions and their leader brokers. It is created by calling <code><a class="el" href="rdkafka_8h.html#ab1dcba74a35e8f3bfe3270ff600581d8" title="Creates a new topic handle for topic named topic. ">rd_kafka_topic_new()</a></code>.</p>
<p>Both <code>rd_kafka_t</code> and <code>rd_kafka_topic_t</code> comes with a configuration API which is optional. Not using the API will cause librdkafka to use its default values which are documented in <a href="https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md">`CONFIGURATION.md`</a>.</p>
<p><b>Note</b>: An application may create multiple <code>rd_kafka_t</code> objects and they share no state.</p>
<p><b>Note</b>: An <code>rd_kafka_topic_t</code> object may only be used with the <code>rd_kafka_t</code> object it was created from.</p>
<h3>Configuration</h3>
<p>To ease integration with the official Apache Kafka software and lower the learning curve, librdkafka implements identical configuration properties as found in the official clients of Apache Kafka.</p>
<p>Configuration is applied prior to object creation using the <code><a class="el" href="rdkafka_8h.html#abb1b319278333e8cdee9442da7f135e8" title="Sets a configuration property. ">rd_kafka_conf_set()</a></code> and <code><a class="el" href="rdkafka_8h.html#ac91b47f7733b324bf4159427e90ccd01" title="Sets a single rd_kafka_topic_conf_t value by property name. ">rd_kafka_topic_conf_set()</a></code> APIs.</p>
<p><b>Note</b>: The <code>rd_kafka.._conf_t</code> objects are not reusable after they have been passed to <code>rd_kafka.._new()</code>. The application does not need to free any config resources after a <code>rd_kafka.._new()</code> call.</p>
<h4>Example</h4>
<pre class="fragment">rd_kafka_conf_t *conf;
char errstr[512];

conf = rd_kafka_conf_new();
rd_kafka_conf_set(conf, "compression.codec", "snappy", errstr, sizeof(errstr));
rd_kafka_conf_set(conf, "batch.num.messages", "100", errstr, sizeof(errstr));

rd_kafka_new(RD_KAFKA_PRODUCER, conf);
</pre><h3>Threads and callbacks</h3>
<p>librdkafka uses multiple threads internally to fully utilize modern hardware. The API is completely thread-safe and the calling application may call any of the API functions from any of its own threads at any time.</p>
<p>A poll-based API is used to provide signaling back to the application, the application should call <a class="el" href="rdkafka_8h.html#ad50c431e3a29d14da534db49bd0682a4" title="Polls the provided kafka handle for events. ">rd_kafka_poll()</a> at regular intervals. The poll API will call the following configured callbacks (optional):</p>
<ul>
<li>message delivery report callback - signals that a message has been delivered or failed delivery, allowing the application to take action and to release any application resources used in the message.</li>
<li>error callback - signals an error. These errors are usually of an informational nature, i.e., failure to connect to a broker, and the application usually does not need to take any action. The type of error is passed as a rd_kafka_resp_err_t enum value, including both remote broker errors as well as local failures.</li>
</ul>
<p>Optional callbacks not triggered by poll, these may be called from any thread:</p>
<ul>
<li>Logging callback - allows the application to output log messages generated by librdkafka.</li>
<li>partitioner callback - application provided message partitioner. The partitioner may be called in any thread at any time, it may be called multiple times for the same key. Partitioner function contraints:<ul>
<li>MUST NOT call any rd_kafka_*() functions</li>
<li>MUST NOT block or execute for prolonged periods of time.</li>
<li>MUST return a value between 0 and partition_cnt-1, or the special RD_KAFKA_PARTITION_UA value if partitioning could not be performed.</li>
</ul>
</li>
</ul>
<h3>Brokers</h3>
<p>librdkafka only needs an initial list of brokers (at least one), called the bootstrap brokers. It will connect to all the bootstrap brokers, specified by the "metadata.broker.list" configuration property or by <code><a class="el" href="rdkafka_8h.html#ab83da8da989fe41693d78d982c7ae6b7" title="Adds one or more brokers to the kafka handle&#39;s list of initial bootstrap brokers. ...">rd_kafka_brokers_add()</a></code>, and query each one for Metadata information which contains the full list of brokers, topic, partitions and their leaders in the Kafka cluster.</p>
<p>Broker names are specified as "host[:port]" where the port is optional (default 9092) and the host is either a resolvable hostname or an IPv4 or IPv6 address. If host resolves to multiple addresses librdkafka will round-robin the addresses for each connection attempt. A DNS record containing all broker address can thus be used to provide a reliable bootstrap broker.</p>
<h3>Feature discovery</h3>
<p>Apache Kafka broker version 0.10.0 added support for the ApiVersionRequest API which allows a client to query a broker for its range of supported API versions.</p>
<p>librdkafka supports this functionality and will query each broker on connect for this information (if <code>api.version.request=true</code>) and use it to enable or disable various protocol features, such as MessageVersion 1 (timestamps), KafkaConsumer, etc.</p>
<p>If the broker fails to respond to the ApiVersionRequest librdkafka will assume the broker is too old to support the API and fall back to an older broker version's API. These fallback versions are hardcoded in librdkafka and is controlled by the <code>broker.version.fallback</code> configuration property.</p>
<h3>Producer API</h3>
<p>After setting up the <code>rd_kafka_t</code> object with type <code>RD_KAFKA_PRODUCER</code> and one or more <code>rd_kafka_topic_t</code> objects librdkafka is ready for accepting messages to be produced and sent to brokers.</p>
<p>The <code><a class="el" href="rdkafka_8h.html#ae24d8ebf1ea15ed8ea0ea40f74662736" title="Produce and send a single message to broker. ">rd_kafka_produce()</a></code> function takes the following arguments:</p>
<ul>
<li><code>rkt</code> - the topic to produce to, previously created with <code><a class="el" href="rdkafka_8h.html#ab1dcba74a35e8f3bfe3270ff600581d8" title="Creates a new topic handle for topic named topic. ">rd_kafka_topic_new()</a></code></li>
<li><code>partition</code> - partition to produce to. If this is set to <code>RD_KAFKA_PARTITION_UA</code> (UnAssigned) then the configured partitioner function will be used to select a target partition.</li>
<li><p class="startli"><code>msgflags</code> - 0, or one of:</p>
<ul>
<li><code>RD_KAFKA_MSG_F_COPY</code> - librdkafka will immediately make a copy of the payload. Use this when the payload is in non-persistent memory, such as the stack.</li>
<li><code>RD_KAFKA_MSG_F_FREE</code> - let librdkafka free the payload using <code>free(3)</code> when it is done with it.</li>
</ul>
<p class="startli">These two flags are mutually exclusive and neither need to be set in which case the payload is neither copied nor freed by librdkafka.</p>
<p class="startli">If <code>RD_KAFKA_MSG_F_COPY</code> flag is not set no data copying will be performed and librdkafka will hold on the payload pointer until the message has been delivered or fails. The delivery report callback will be called when librdkafka is done with the message to let the application regain ownership of the payload memory. The application must not free the payload in the delivery report callback if <code>RD_KAFKA_MSG_F_FREE is set</code>.</p>
</li>
<li><code>payload</code>,<code>len</code> - the message payload</li>
<li><code>key</code>,<code>keylen</code> - an optional message key which can be used for partitioning. It will be passed to the topic partitioner callback, if any, and will be attached to the message when sending to the broker.</li>
<li><code>msg_opaque</code> - an optional application-provided per-message opaque pointer that will be provided in the message delivery callback to let the application reference a specific message.</li>
</ul>
<p><code><a class="el" href="rdkafka_8h.html#ae24d8ebf1ea15ed8ea0ea40f74662736" title="Produce and send a single message to broker. ">rd_kafka_produce()</a></code> is a non-blocking API, it will enqueue the message on an internal queue and return immediately. If the number of queued messages would exceed the "queue.buffering.max.messages" configuration property then <code><a class="el" href="rdkafka_8h.html#ae24d8ebf1ea15ed8ea0ea40f74662736" title="Produce and send a single message to broker. ">rd_kafka_produce()</a></code> returns -1 and sets errno to <code>ENOBUFS</code>, thus providing a backpressure mechanism.</p>
<p><b>Note</b>: See <code>examples/rdkafka_performance.c</code> for a producer implementation.</p>
<h3>Simple Consumer API (legacy)</h3>
<p>NOTE: For the high-level KafkaConsumer interface see rd_kafka_subscribe (<a class="el" href="rdkafka_8h.html" title="Apache Kafka C/C++ consumer and producer client library. ">rdkafka.h</a>) or KafkaConsumer (<a class="el" href="rdkafkacpp_8h.html" title="Apache Kafka C/C++ consumer and producer client library. ">rdkafkacpp.h</a>)</p>
<p>The consumer API is a bit more stateful than the producer API. After creating <code>rd_kafka_t</code> with type <code>RD_KAFKA_CONSUMER</code> and <code>rd_kafka_topic_t</code> instances the application must also start the consumer for a given partition by calling <code><a class="el" href="rdkafka_8h.html#ae21dcd2d8c6195baf7f9f4952d7e12d4" title="Start consuming messages for topic rkt and partition at offset offset which may either be an absolute...">rd_kafka_consume_start()</a></code>.</p>
<p><code><a class="el" href="rdkafka_8h.html#ae21dcd2d8c6195baf7f9f4952d7e12d4" title="Start consuming messages for topic rkt and partition at offset offset which may either be an absolute...">rd_kafka_consume_start()</a></code> arguments:</p>
<ul>
<li><code>rkt</code> - the topic to start consuming from, previously created with <code><a class="el" href="rdkafka_8h.html#ab1dcba74a35e8f3bfe3270ff600581d8" title="Creates a new topic handle for topic named topic. ">rd_kafka_topic_new()</a></code>.</li>
<li><code>partition</code> - partition to consume from.</li>
<li><code>offset</code> - message offset to start consuming from. This may either be an absolute message offset or one of the two special offsets: <code>RD_KAFKA_OFFSET_BEGINNING</code> to start consuming from the beginning of the partition's queue (oldest message), or <code>RD_KAFKA_OFFSET_END</code> to start consuming at the next message to be produced to the partition, or <code>RD_KAFKA_OFFSET_STORED</code> to use the offset store.</li>
</ul>
<p>After a topic+partition consumer has been started librdkafka will attempt to keep "queued.min.messages" messages in the local queue by repeatedly fetching batches of messages from the broker.</p>
<p>This local message queue is then served to the application through three different consume APIs:</p>
<ul>
<li><code><a class="el" href="rdkafka_8h.html#aa49d14e8b742365f9f25d35318ff0b7e" title="Consume a single message from topic rkt and partition. ">rd_kafka_consume()</a></code> - consumes a single message</li>
<li><code><a class="el" href="rdkafka_8h.html#a53511739a2cf498b8d88287fef6873ce" title="Consume up to rkmessages_size from topic rkt and partition putting a pointer to each message in the a...">rd_kafka_consume_batch()</a></code> - consumes one or more messages</li>
<li><code><a class="el" href="rdkafka_8h.html#a303fa0f0da7f3c28bed35570adc983c6" title="Consumes messages from topic rkt and partition, calling the provided callback for each consumed messs...">rd_kafka_consume_callback()</a></code> - consumes all messages in the local queue and calls a callback function for each one.</li>
</ul>
<p>These three APIs are listed above the ascending order of performance, <code><a class="el" href="rdkafka_8h.html#aa49d14e8b742365f9f25d35318ff0b7e" title="Consume a single message from topic rkt and partition. ">rd_kafka_consume()</a></code> being the slowest and <code><a class="el" href="rdkafka_8h.html#a303fa0f0da7f3c28bed35570adc983c6" title="Consumes messages from topic rkt and partition, calling the provided callback for each consumed messs...">rd_kafka_consume_callback()</a></code> being the fastest. The different consume variants are provided to cater for different application needs.</p>
<p>A consumed message, as provided or returned by each of the consume functions, is represented by the <code><a class="el" href="structrd__kafka__message__t.html" title="A Kafka message as returned by the rd_kafka_consume*() family of functions as well as provided to the...">rd_kafka_message_t</a></code> type.</p>
<p><code><a class="el" href="structrd__kafka__message__t.html" title="A Kafka message as returned by the rd_kafka_consume*() family of functions as well as provided to the...">rd_kafka_message_t</a></code> members:</p>
<ul>
<li><code>err</code> - Error signaling back to the application. If this field is non-zero the <code>payload</code> field should be considered an error message and <code>err</code> is an error code (<code>rd_kafka_resp_err_t</code>). If <code>err</code> is zero then the message is a proper fetched message and <code>payload</code> et.al contains message payload data.</li>
<li><code>rkt</code>,<code>partition</code> - Topic and partition for this message or error.</li>
<li><code>payload</code>,<code>len</code> - Message payload data or error message (err!=0).</li>
<li><code>key</code>,<code>key_len</code> - Optional message key as specified by the producer</li>
<li><code>offset</code> - Message offset</li>
</ul>
<p>Both the <code>payload</code> and <code>key</code> memory, as well as the message as a whole, is owned by librdkafka and must not be used after an <code><a class="el" href="rdkafka_8h.html#a35e0c519209d1afe9e29468f766c1c24" title="Frees resources for rkmessage and hands ownership back to rdkafka. ">rd_kafka_message_destroy()</a></code> call. librdkafka will share the same messageset receive buffer memory for all message payloads of that messageset to avoid excessive copying which means that if the application decides to hang on to a single <code><a class="el" href="structrd__kafka__message__t.html" title="A Kafka message as returned by the rd_kafka_consume*() family of functions as well as provided to the...">rd_kafka_message_t</a></code> it will hinder the backing memory to be released for all other messages from the same messageset.</p>
<p>When the application is done consuming messages from a topic+partition it should call <code><a class="el" href="rdkafka_8h.html#acf07475e5e85e63fc5321a1087288cd4" title="Stop consuming messages for topic rkt and partition, purging all messages currently in the local queu...">rd_kafka_consume_stop()</a></code> to stop the consumer. This will also purge any messages currently in the local queue.</p>
<p><b>Note</b>: See <code>examples/rdkafka_performance.c</code> for a consumer implementation.</p>
<h4>Offset management</h4>
<p>Broker based offset management is available for broker version &gt;= 0.9.0 in conjunction with using the high-level KafkaConsumer interface (see <a class="el" href="rdkafka_8h.html" title="Apache Kafka C/C++ consumer and producer client library. ">rdkafka.h</a> or <a class="el" href="rdkafkacpp_8h.html" title="Apache Kafka C/C++ consumer and producer client library. ">rdkafkacpp.h</a>)</p>
<p>Offset management is also available through a local offset file store, where the offset is periodically written to a local file for each topic+partition according to the following topic configuration properties:</p>
<ul>
<li><code>auto.commit.enable</code></li>
<li><code>auto.commit.interval.ms</code></li>
<li><code>offset.store.path</code></li>
<li><code>offset.store.sync.interval.ms</code></li>
</ul>
<p>There is currently no support for offset management with ZooKeeper.</p>
<h4>Consumer groups</h4>
<p>Broker based consumer groups (requires Apache Kafka broker &gt;=0.9) are supported, see KafkaConsumer in <a class="el" href="rdkafka_8h.html" title="Apache Kafka C/C++ consumer and producer client library. ">rdkafka.h</a> or <a class="el" href="rdkafkacpp_8h.html" title="Apache Kafka C/C++ consumer and producer client library. ">rdkafkacpp.h</a></p>
<h3>Topics</h3>
<h4>Topic auto creation</h4>
<p>Topic auto creation is supported by librdkafka. The broker needs to be configured with "auto.create.topics.enable=true".</p>
<h3>Metadata</h3>
<h4>&lt; 0.9.3</h4>
<p>Previous to the 0.9.3 release librdkafka's metadata handling was chatty and excessive, which usually isn't a problem in small to medium-sized clusters, but in large clusters with a large amount of librdkafka clients the metadata requests could hog broker CPU and bandwidth.</p>
<h4>&gt; 0.9.3</h4>
<p>The remaining Metadata sections describe the current behaviour.</p>
<p><b>Note:</b> "Known topics" in the following section means topics for locally created <code>rd_kafka_topic_t</code> objects.</p>
<h4>Query reasons</h4>
<p>There are four reasons to query metadata:</p>
<ul>
<li>brokers - update/populate cluster broker list, so the client can find and connect to any new brokers added.</li>
<li>specific topic - find leader or partition count for specific topic</li>
<li>known topics - same, but for all locally known topics.</li>
<li>all topics - get topic names for consumer group wildcard subscription matching</li>
</ul>
<p>The above list is sorted so that the sub-sequent entries contain the information above, e.g., 'known topics' contains enough information to also satisfy 'specific topic' and 'brokers'.</p>
<h4>Caching strategy</h4>
<p>The prevalent cache timeout is <code>metadata.max.age.ms</code>, any cached entry will remain authoritative for this long or until a relevant broker error is returned.</p>
<ul>
<li>brokers - eternally cached, the broker list is additative.</li>
<li>topics - cached for <code>metadata.max.age.ms</code></li>
</ul>
<h2>Appendix</h2>
<h3>Test details</h3>
<h4>Test1: Produce to two brokers, two partitions, required.acks=2, 100 byte messages</h4>
<p>Each broker is leader for one of the two partitions. The random partitioner is used (default) and each broker and partition is assigned approximately 250000 messages each.</p>
<p><b>Command:</b> </p>
<pre class="fragment"># examples/rdkafka_performance -P -t test2 -s 100 -c 500000 -m "_____________Test1:TwoBrokers:500kmsgs:100bytes" -S 1 -a 2
....
% 500000 messages and 50000000 bytes sent in 587ms: 851531 msgs/s and 85.15 Mb/s, 0 messages failed, no compression
</pre><p><b>Result:</b></p>
<p>Message transfer rate is approximately <b>850000 messages per second</b>, <b>85 megabytes per second</b>.</p>
<h4>Test2: Produce to one broker, one partition, required.acks=0, 100 byte messages</h4>
<p><b>Command:</b> </p>
<pre class="fragment"># examples/rdkafka_performance -P -t test2 -s 100 -c 500000 -m "_____________Test2:OneBrokers:500kmsgs:100bytes" -S 1 -a 0 -p 1
....
% 500000 messages and 50000000 bytes sent in 698ms: 715994 msgs/s and 71.60 Mb/s, 0 messages failed, no compression
</pre><p><b>Result:</b></p>
<p>Message transfer rate is approximately <b>710000 messages per second</b>, <b>71 megabytes per second</b>.</p>
<h4>Test3: Produce to two brokers, two partitions, required.acks=2, 100 byte messages, snappy compression</h4>
<p><b>Command:</b> </p>
<pre class="fragment"># examples/rdkafka_performance -P -t test2 -s 100 -c 500000 -m "_____________Test3:TwoBrokers:500kmsgs:100bytes:snappy" -S 1 -a 2 -z snappy
....
% 500000 messages and 50000000 bytes sent in 1672ms: 298915 msgs/s and 29.89 Mb/s, 0 messages failed, snappy compression
</pre><p><b>Result:</b></p>
<p>Message transfer rate is approximately <b>300000 messages per second</b>, <b>30 megabytes per second</b>.</p>
<h4>Test4: Produce to two brokers, two partitions, required.acks=2, 100 byte messages, gzip compression</h4>
<p><b>Command:</b> </p>
<pre class="fragment"># examples/rdkafka_performance -P -t test2 -s 100 -c 500000 -m "_____________Test3:TwoBrokers:500kmsgs:100bytes:gzip" -S 1 -a 2 -z gzip
....
% 500000 messages and 50000000 bytes sent in 2111ms: 236812 msgs/s and 23.68 Mb/s, 0 messages failed, gzip compression
</pre><p><b>Result:</b></p>
<p>Message transfer rate is approximately <b>230000 messages per second</b>, <b>23 megabytes per second</b>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="INTRODUCTION_8md.html">INTRODUCTION.md</a></li>
    <li class="footer">Generated on Thu Aug 3 2017 14:04:47 for librdkafka by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.5 </li>
  </ul>
</div>
</body>
</html>
